{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c157a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1b6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversations_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        conversations = file.readlines()\n",
    "    return conversations\n",
    "\n",
    "def process_text(text):\n",
    "    # Remove 'Patient: ' and 'Doctor: ' from the text\n",
    "    text = text.replace('Patient: ', '').replace('Doctor: ', '')\n",
    "    return text\n",
    "\n",
    "def format_data_for_rl(conversations):\n",
    "    dialogue_pairs = []\n",
    "\n",
    "    patient_text = None\n",
    "    for line in conversations:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Patient:\"):\n",
    "            patient_text = process_text(line)\n",
    "        elif line.startswith(\"Doctor:\") and patient_text is not None:\n",
    "            doctor_text = process_text(line)\n",
    "            dialogue_pairs.append([patient_text, doctor_text])\n",
    "\n",
    "    return dialogue_pairs\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'datasets/patient-doctor.txt'\n",
    "conversations = read_conversations_from_file(file_path)\n",
    "data_array = format_data_for_rl(conversations)\n",
    "for i,o in data_array[:3]:\n",
    "    print('patient:',i)\n",
    "    print('doctor:',o)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0333a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the text data for tokenization\n",
    "all_text = [patient_text + \" \" + doctor_text for patient_text, doctor_text in data_array]\n",
    "\n",
    "# Tokenize the data and build the vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\",vocab_size)\n",
    "# Convert text data to integer sequences\n",
    "sequences = [tokenizer.texts_to_sequences([patient_text, doctor_text]) for patient_text, doctor_text in data_array]\n",
    "\n",
    "# Get the maximum sequence length\n",
    "max_sequence_length = max(max(len(seq[0]), len(seq[1])) for seq in sequences)\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences([seq[0] for seq in sequences], maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences([seq[1] for seq in sequences], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "print(input_sequences[0])\n",
    "print(target_sequences[0])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic chatbot model using LSTM\n",
    "class ChatbotModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "def create_language_model(vocab_size, embedding_dim, lstm_units, max_sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "class ChatbotModel_Attention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(ChatbotModel_Attention, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        rnn_output = self.rnn(x)\n",
    "        context_vector = self.attention([rnn_output, rnn_output])\n",
    "        output = self.output_layer(context_vector)\n",
    "        return output\n",
    "    \n",
    "# Chatbot model using Bi-directional LSTM with attention layer and beamsearch setup\n",
    "class ChatbotModel_BiDirection(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, bidirectional=True, attention=False, beam_width=3):\n",
    "        super(ChatbotModel_BiDirection, self).__init__()  # Use tf.keras.Model as the superclass\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        if bidirectional:\n",
    "            self.rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True))\n",
    "        else:\n",
    "            self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
    "        if attention:\n",
    "            self.attention = tf.keras.layers.Attention()\n",
    "        else:\n",
    "            self.attention = None\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.rnn(x)\n",
    "        if self.attention:\n",
    "            x = self.attention([x, x])\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    \n",
    "    def beam_search(self, inputs, max_length, initial_state=None):\n",
    "        # Beam search implementation to generate multiple likely responses\n",
    "        rnn_inputs = self.embedding(inputs)\n",
    "        rnn_outputs = self.rnn(rnn_inputs)\n",
    "        \n",
    "        if self.attention:\n",
    "            rnn_outputs = self.attention([rnn_outputs, rnn_outputs])\n",
    "\n",
    "        # Get the states from the forward LSTM\n",
    "        forward_lstm = self.rnn.forward_layer\n",
    "        forward_state_h = rnn_outputs[:, :, :forward_lstm.units]\n",
    "        forward_state_c = rnn_outputs[:, :, forward_lstm.units: 2 * forward_lstm.units]\n",
    "        \n",
    "        # Get the states from the backward LSTM\n",
    "        backward_lstm = self.rnn.backward_layer\n",
    "        backward_state_h = rnn_outputs[:, :, 2 * backward_lstm.units: 3 * backward_lstm.units]\n",
    "        backward_state_c = rnn_outputs[:, :, 3 * backward_lstm.units:]\n",
    "\n",
    "        forward_state = [forward_state_h, forward_state_c]\n",
    "        backward_state = [backward_state_h, backward_state_c]\n",
    "        rnn_states = [forward_state, backward_state]\n",
    "\n",
    "        beam_search_decoder = tfa.seq2seq.BeamSearchDecoder(\n",
    "            cell=self.rnn.forward_layer.cell,\n",
    "            beam_width=self.beam_width,\n",
    "            output_layer=self.output_layer,\n",
    "            maximum_iterations=max_length,\n",
    "        )\n",
    "        output, _, _ = beam_search_decoder(\n",
    "            rnn_outputs, sequence_length=None, initial_state=rnn_states\n",
    "        )\n",
    "        return output.predicted_ids[:, :, 0]\n",
    "    \n",
    "# Initialize and compile the chatbot model\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "chatbot_model = ChatbotModel(vocab_size, embedding_dim, rnn_units)\n",
    "chatbot_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5679722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between reference and generated responses\n",
    "def compute_cosine_similarity(references, responses):\n",
    "    # Convert the reference and generated responses to numerical embeddings\n",
    "    reference_embeddings = chatbot_model.embedding(tf.constant(references))\n",
    "    response_embeddings = chatbot_model.embedding(tf.constant(responses))\n",
    "    \n",
    "    # Flatten the embeddings to prepare them for similarity computation\n",
    "    reference_flat = tf.keras.layers.Flatten()(reference_embeddings)\n",
    "    response_flat = tf.keras.layers.Flatten()(response_embeddings)\n",
    "\n",
    "    # Compute cosine similarity between the flattened embeddings\n",
    "    similarity = cosine_similarity(reference_flat.numpy(), response_flat.numpy())\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Reward function\n",
    "def reward_function(references, responses, model):\n",
    "    cosine_similarities = compute_cosine_similarity(references, responses)\n",
    "    rewards = (cosine_similarities + 1) / 2  # Normalize cosine similarity to [0, 1]\n",
    "\n",
    "    # Convert rewards to tensor\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "\n",
    "    # Reshape the rewards for broadcasting during the gradient update\n",
    "    rewards = tf.reshape(rewards, (-1, 1))\n",
    "\n",
    "    # Calculate log probabilities of actions (generated responses)\n",
    "    actions = tf.argmax(responses, axis=-1)\n",
    "    log_probs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=actions, logits=responses)\n",
    "\n",
    "    # Apply rewards to the log probabilities (policy gradient)\n",
    "    rewards = tf.stop_gradient(rewards)\n",
    "    policy_gradient = -log_probs * rewards\n",
    "\n",
    "    return policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcac306a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Add a time dimension to the input tensor\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     batch_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(batch_inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m chatbot_model(batch_inputs)\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m chatbot_model\u001b[38;5;241m.\u001b[39mcompiled_loss(batch_targets, logits)\n\u001b[0;32m     15\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, chatbot_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training loop with reward-based policy gradient update\n",
    "num_epochs = 50\n",
    "patience = 7\n",
    "prev_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    for batch_inputs, batch_targets in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Add a time dimension to the input tensor\n",
    "            batch_inputs = tf.expand_dims(batch_inputs, axis=-1)\n",
    "            logits, _ = chatbot_model(batch_inputs)\n",
    "            loss = chatbot_model.compiled_loss(batch_targets, logits)\n",
    "            grads = tape.gradient(loss, chatbot_model.trainable_variables)\n",
    "\n",
    "            # Compute reward and update loss\n",
    "            generated_responses, _ = chatbot_model(batch_inputs)\n",
    "            rewards = reward_function(batch_targets, generated_responses, chatbot_model)\n",
    "            grads = [g * r for g, r in zip(grads, rewards)]\n",
    "\n",
    "            chatbot_model.optimizer.apply_gradients(zip(grads, chatbot_model.trainable_variables))\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = chatbot_model.evaluate(dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if val_loss >= prev_val_loss:\n",
    "        early_stopping_counter += 1\n",
    "    else:\n",
    "        early_stopping_counter = 0\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping to prevent overfitting.\")\n",
    "        break\n",
    "    prev_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b2d63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/reinforced_saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/reinforced_saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "chatbot_model.save_weights(\"models/reinforced_trained_model_weights.h5\")\n",
    "chatbot_model.save(\"models/reinforced_saved_model\", save_format=\"tf\")\n",
    "\n",
    "# Save the tokenizer\n",
    "with open(\"models/reinforced_tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05570e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"models/reinforced_saved_model\")\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"models/Attention_tokenizer.pkl\", \"rb\") as file:\n",
    "    loaded_tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "657c39ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:Hello, I am feeling depressed\n",
      "Bot: necessary address looking build all, emotions, fingers. redirect pep authenticity, hurt environment. tricky. less effective traumas progress, thrive definitely! present\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"User:\")\n",
    "\n",
    "while inp != '':\n",
    "    print(\"Bot:\",generate_response(inp, chatbot_model, tokenizer))\n",
    "    inp = input(\"User:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
